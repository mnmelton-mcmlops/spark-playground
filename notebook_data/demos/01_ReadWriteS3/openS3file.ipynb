{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c6304a6-8187-49b7-b2d2-11f25cb3c61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession, Catalog\n",
    "from pyspark.sql import DataFrame, DataFrameStatFunctions, DataFrameNaFunctions\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql.types import Row\n",
    "from subprocess import check_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d412f01-6db5-4651-b4ff-a5a8a41a6eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/bitnami/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "com.amazonaws#aws-java-sdk-bundle added as a dependency\n",
      "org.apache.spark#spark-hadoop-cloud_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-82b967da-0825-4a92-8edf-ce3ffd701f1c;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.2.0 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.704 in central\n",
      "\tfound org.apache.spark#spark-hadoop-cloud_2.12;3.3.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.2 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.2 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.4 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.32 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.2 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.1026 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      "\tfound org.apache.hadoop#hadoop-openstack;3.3.2 in central\n",
      "\tfound org.apache.hadoop#hadoop-annotations;3.3.2 in central\n",
      "\tfound org.apache.httpcomponents#httpcore;4.4.14 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-annotations;2.13.3 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-databind;2.13.3 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-core;2.13.3 in central\n",
      "\tfound joda-time#joda-time;2.10.13 in central\n",
      "\tfound com.fasterxml.jackson.dataformat#jackson-dataformat-cbor;2.13.3 in central\n",
      "\tfound org.apache.httpcomponents#httpclient;4.5.13 in central\n",
      "\tfound commons-codec#commons-codec;1.15 in central\n",
      "\tfound org.apache.hadoop#hadoop-azure;3.3.2 in central\n",
      "\tfound com.microsoft.azure#azure-storage;7.0.1 in central\n",
      "\tfound com.microsoft.azure#azure-keyvault-core;1.0.0 in central\n",
      "\tfound org.apache.hadoop.thirdparty#hadoop-shaded-guava;1.1.1 in central\n",
      "\tfound org.codehaus.jackson#jackson-mapper-asl;1.9.13 in central\n",
      "\tfound org.codehaus.jackson#jackson-core-asl;1.9.13 in central\n",
      "\tfound org.apache.hadoop#hadoop-cloud-storage;3.3.2 in central\n",
      "\tfound org.apache.hadoop#hadoop-aliyun;3.3.2 in central\n",
      "\tfound com.aliyun.oss#aliyun-sdk-oss;3.13.0 in central\n",
      "\tfound org.jdom#jdom2;2.0.6 in central\n",
      "\tfound org.codehaus.jettison#jettison;1.1 in central\n",
      "\tfound stax#stax-api;1.0.1 in central\n",
      "\tfound com.aliyun#aliyun-java-sdk-core;4.5.10 in central\n",
      "\tfound com.google.code.gson#gson;2.8.9 in central\n",
      "\tfound javax.xml.bind#jaxb-api;2.2.11 in central\n",
      "\tfound org.ini4j#ini4j;0.5.4 in central\n",
      "\tfound io.opentracing#opentracing-api;0.33.0 in central\n",
      "\tfound io.opentracing#opentracing-util;0.33.0 in central\n",
      "\tfound io.opentracing#opentracing-noop;0.33.0 in central\n",
      "\tfound com.aliyun#aliyun-java-sdk-ram;3.1.0 in central\n",
      "\tfound com.aliyun#aliyun-java-sdk-kms;2.11.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-azure-datalake;3.3.2 in central\n",
      "\tfound com.microsoft.azure#azure-data-lake-store-sdk;2.3.9 in central\n",
      "\tfound org.apache.hadoop#hadoop-cos;3.3.2 in central\n",
      "\tfound com.qcloud#cos_api-bundle;5.6.19 in central\n",
      "\tfound org.eclipse.jetty#jetty-util-ajax;9.4.46.v20220331 in central\n",
      "\tfound org.eclipse.jetty#jetty-util;9.4.46.v20220331 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      ":: resolution report :: resolve 1158ms :: artifacts dl 71ms\n",
      "\t:: modules in use:\n",
      "\tcom.aliyun#aliyun-java-sdk-core;4.5.10 from central in [default]\n",
      "\tcom.aliyun#aliyun-java-sdk-kms;2.11.0 from central in [default]\n",
      "\tcom.aliyun#aliyun-java-sdk-ram;3.1.0 from central in [default]\n",
      "\tcom.aliyun.oss#aliyun-sdk-oss;3.13.0 from central in [default]\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.1026 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-annotations;2.13.3 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-core;2.13.3 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-databind;2.13.3 from central in [default]\n",
      "\tcom.fasterxml.jackson.dataformat#jackson-dataformat-cbor;2.13.3 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.8.9 from central in [default]\n",
      "\tcom.microsoft.azure#azure-data-lake-store-sdk;2.3.9 from central in [default]\n",
      "\tcom.microsoft.azure#azure-keyvault-core;1.0.0 from central in [default]\n",
      "\tcom.microsoft.azure#azure-storage;7.0.1 from central in [default]\n",
      "\tcom.qcloud#cos_api-bundle;5.6.19 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.15 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\tio.opentracing#opentracing-api;0.33.0 from central in [default]\n",
      "\tio.opentracing#opentracing-noop;0.33.0 from central in [default]\n",
      "\tio.opentracing#opentracing-util;0.33.0 from central in [default]\n",
      "\tjavax.xml.bind#jaxb-api;2.2.11 from central in [default]\n",
      "\tjoda-time#joda-time;2.10.13 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aliyun;3.3.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-annotations;3.3.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-azure;3.3.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-azure-datalake;3.3.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-cloud-storage;3.3.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-cos;3.3.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-openstack;3.3.2 from central in [default]\n",
      "\torg.apache.hadoop.thirdparty#hadoop-shaded-guava;1.1.1 from central in [default]\n",
      "\torg.apache.httpcomponents#httpclient;4.5.13 from central in [default]\n",
      "\torg.apache.httpcomponents#httpcore;4.4.14 from central in [default]\n",
      "\torg.apache.spark#spark-hadoop-cloud_2.12;3.3.0 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-core-asl;1.9.13 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-mapper-asl;1.9.13 from central in [default]\n",
      "\torg.codehaus.jettison#jettison;1.1 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-util;9.4.46.v20220331 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-util-ajax;9.4.46.v20220331 from central in [default]\n",
      "\torg.ini4j#ini4j;0.5.4 from central in [default]\n",
      "\torg.jdom#jdom2;2.0.6 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.32 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.4 from central in [default]\n",
      "\tstax#stax-api;1.0.1 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\torg.apache.hadoop#hadoop-aws;3.2.0 by [org.apache.hadoop#hadoop-aws;3.3.2] in [default]\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.375 by [com.amazonaws#aws-java-sdk-bundle;1.11.704] in [default]\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.704 by [com.amazonaws#aws-java-sdk-bundle;1.11.1026] in [default]\n",
      "\torg.eclipse.jetty#jetty-util-ajax;9.4.43.v20210629 by [org.eclipse.jetty#jetty-util-ajax;9.4.46.v20220331] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   52  |   0   |   0   |   4   ||   48  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-82b967da-0825-4a92-8edf-ce3ffd701f1c\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 48 already retrieved (0kB/19ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/03 20:49:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/03 20:49:28 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-------+-----+-------+-------+-------+-----+------------------+--------+\n",
      "|LatD | \"LatM\"| \"LatS\"| \"NS\"| \"LonD\"| \"LonM\"| \"LonS\"| \"EW\"| \"City\"           | \"State\"|\n",
      "+-----+-------+-------+-----+-------+-------+-------+-----+------------------+--------+\n",
      "|   41|    5  |   59  | \"N\" |     80|   39  |    0  | \"W\" | \"Youngstown\"     | OH     |\n",
      "|   42|   52  |   48  | \"N\" |     97|   23  |   23  | \"W\" | \"Yankton\"        | SD     |\n",
      "|   46|   35  |   59  | \"N\" |    120|   30  |   36  | \"W\" | \"Yakima\"         | WA     |\n",
      "|   42|   16  |   12  | \"N\" |     71|   48  |    0  | \"W\" | \"Worcester\"      | MA     |\n",
      "|   43|   37  |   48  | \"N\" |     89|   46  |   11  | \"W\" | \"Wisconsin Dells\"| WI     |\n",
      "|   36|    5  |   59  | \"N\" |     80|   15  |    0  | \"W\" | \"Winston-Salem\"  | NC     |\n",
      "|   49|   52  |   48  | \"N\" |     97|    9  |    0  | \"W\" | \"Winnipeg\"       | MB     |\n",
      "|   39|   11  |   23  | \"N\" |     78|    9  |   36  | \"W\" | \"Winchester\"     | VA     |\n",
      "|   34|   14  |   24  | \"N\" |     77|   55  |   11  | \"W\" | \"Wilmington\"     | NC     |\n",
      "|   39|   45  |    0  | \"N\" |     75|   33  |    0  | \"W\" | \"Wilmington\"     | DE     |\n",
      "|   48|    9  |    0  | \"N\" |    103|   37  |   12  | \"W\" | \"Williston\"      | ND     |\n",
      "|   41|   15  |    0  | \"N\" |     77|    0  |    0  | \"W\" | \"Williamsport\"   | PA     |\n",
      "|   37|   40  |   48  | \"N\" |     82|   16  |   47  | \"W\" | \"Williamson\"     | WV     |\n",
      "|   33|   54  |    0  | \"N\" |     98|   29  |   23  | \"W\" | \"Wichita Falls\"  | TX     |\n",
      "|   37|   41  |   23  | \"N\" |     97|   20  |   23  | \"W\" | \"Wichita\"        | KS     |\n",
      "|   40|    4  |   11  | \"N\" |     80|   43  |   12  | \"W\" | \"Wheeling\"       | WV     |\n",
      "|   26|   43  |   11  | \"N\" |     80|    3  |    0  | \"W\" | \"West Palm Beach\"| FL     |\n",
      "|   47|   25  |   11  | \"N\" |    120|   19  |   11  | \"W\" | \"Wenatchee\"      | WA     |\n",
      "|   41|   25  |   11  | \"N\" |    122|   23  |   23  | \"W\" | \"Weed\"           | CA     |\n",
      "|   31|   13  |   11  | \"N\" |     82|   20  |   59  | \"W\" | \"Waycross\"       | GA     |\n",
      "+-----+-------+-------+-----+-------+-------+-------+-----+------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SPARK_DRIVER_HOST = check_output([\"hostname\", \"-i\"]).decode(encoding=\"utf-8\").strip()\n",
    "spark_conf = SparkConf()\n",
    "spark_conf.setAll([\n",
    "    ('spark.master', 'spark://spark:7077'),\n",
    "    ('spark.app.name', 'myApp'),\n",
    "    ('spark.submit.deployMode', 'client'),\n",
    "    ('spark.ui.showConsoleProgress', 'true'),\n",
    "    ('spark.eventLog.enabled', 'false'),\n",
    "    ('spark.logConf', 'false'),\n",
    "    ('spark.driver.bindAddress', '0.0.0.0'),\n",
    "    ('spark.driver.host', SPARK_DRIVER_HOST),\n",
    "    ('spark.jars.packages', 'org.apache.hadoop:hadoop-aws:3.2.0,com.amazonaws:aws-java-sdk-bundle:1.11.704,org.apache.spark:spark-hadoop-cloud_2.12:3.3.0'),\n",
    "    (\"spark.hadoop.fs.s3a.endpoint\", 'http://minio:9000'),\n",
    "    ('spark.hadoop.fs.s3a.access.key', 'minio-root-user'),\n",
    "    ('spark.hadoop.fs.s3a.secret.key', 'minio-root-password'),\n",
    "    ('spark.hadoop.fs.s3a.bucket.all.committer.magic.enabled', True),\n",
    "    (\"spark.hadoop.fs.s3a.fast.upload\", True),\n",
    "    (\"spark.hadoop.fs.s3a.path.style.access\", True),\n",
    "    (\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "])\n",
    " \n",
    "spark_sess          = SparkSession.builder.config(conf=spark_conf).getOrCreate()\n",
    "spark_ctxt          = spark_sess.sparkContext\n",
    "spark_reader        = spark_sess.read\n",
    "spark_streamReader  = spark_sess.readStream\n",
    "spark_ctxt.setLogLevel(\"WARN\")\n",
    "\n",
    "citiesDF = spark_sess.read.option(\"header\",True).csv('s3a://cities/cities.csv')\n",
    "\n",
    "citiesDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81e3f4e9-0235-48ed-8b4e-a948c9416dcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LatD',\n",
       " ' \"LatM\"',\n",
       " ' \"LatS\"',\n",
       " ' \"NS\"',\n",
       " ' \"LonD\"',\n",
       " ' \"LonM\"',\n",
       " ' \"LonS\"',\n",
       " ' \"EW\"',\n",
       " ' \"City\"',\n",
       " ' \"State\"']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "citiesDF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83e7d1b9-6ae7-479d-8a2a-92f81cb575e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[LatM: string, LonM: string]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanCitiesDF = citiesDF \\\n",
    "   .withColumnRenamed(' \"LatM\"', 'LatM') \\\n",
    "   .withColumnRenamed(' \"LonM\"', 'LonM') \\\n",
    "   .select('LatM', 'LonM')\n",
    "cleanCitiesDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46aa6eea-04fc-4531-aaeb-d126190720df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "| LatM| LonM|\n",
      "+-----+-----+\n",
      "|    5|   39|\n",
      "|   52|   23|\n",
      "|   35|   30|\n",
      "|   16|   48|\n",
      "|   37|   46|\n",
      "|    5|   15|\n",
      "|   52|    9|\n",
      "|   11|    9|\n",
      "|   14|   55|\n",
      "|   45|   33|\n",
      "|    9|   37|\n",
      "|   15|    0|\n",
      "|   40|   16|\n",
      "|   54|   29|\n",
      "|   41|   20|\n",
      "|    4|   43|\n",
      "|   43|    3|\n",
      "|   25|   19|\n",
      "|   25|   23|\n",
      "|   13|   20|\n",
      "+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleanCitiesDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3fefb972-35cd-44b6-8947-64c881342c87",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "path s3a://cities/cleanCities4.csv already exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m cleanCitiesDF\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheader\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms3a://cities/cleanCities4.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~opt/bitnami/spark/python/pyspark/sql/readwriter.py:968\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    966\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msave()\n\u001b[1;32m    967\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 968\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~opt/bitnami/spark/venv/lib/python3.8/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~opt/bitnami/spark/python/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: path s3a://cities/cleanCities4.csv already exists."
     ]
    }
   ],
   "source": [
    "cleanCitiesDF.write.format(\"csv\").option(\"header\",  True).save(\"s3a://cities/cleanCities4.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61d3162c-9b1d-4837-a1be-c5a8689d16e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|LatM|LonM|\n",
      "+----+----+\n",
      "|   5|  39|\n",
      "|  52|  23|\n",
      "|  35|  30|\n",
      "|  16|  48|\n",
      "|  37|  46|\n",
      "|   5|  15|\n",
      "|  52|   9|\n",
      "|  11|   9|\n",
      "|  14|  55|\n",
      "|  45|  33|\n",
      "|   9|  37|\n",
      "|  15|   0|\n",
      "|  40|  16|\n",
      "|  54|  29|\n",
      "|  41|  20|\n",
      "|   4|  43|\n",
      "|  43|   3|\n",
      "|  25|  19|\n",
      "|  25|  23|\n",
      "|  13|  20|\n",
      "+----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "citiesDFRecovered = spark_sess.read.option(\"header\",True).csv('s3a://cities/cleanCities4.csv')\n",
    "citiesDFRecovered.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8155b14-24ec-4db5-8d93-1c4909e5da76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+------------------+\n",
      "|    name|      city|           stadium|\n",
      "+--------+----------+------------------+\n",
      "| Bengals|Cincinnati|Paul Brown Stadium|\n",
      "|Steelers|Pittsburgh|       Heinz Field|\n",
      "|  Browns| Cleveland| FirstEnergy Field|\n",
      "|  Ravens| Baltimore|  M&T Bank Stadium|\n",
      "+--------+----------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "Team = Row(\"name\", \"city\", \"stadium\")\n",
    "afcNorth = [Team(\"Bengals\", \"Cincinnati\", \"Paul Brown Stadium\"),\n",
    "            Team(\"Steelers\", \"Pittsburgh\", \"Heinz Field\"),\n",
    "            Team(\"Browns\", \"Cleveland\", \"FirstEnergy Field\"),\n",
    "            Team(\"Ravens\", \"Baltimore\", \"M&T Bank Stadium\")]\n",
    "afcNorthDataFrame = spark_sess.createDataFrame(afcNorth)\n",
    "afcNorthDataFrame.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1aded75-d3c2-4d79-b964-ddb8d0fc4651",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "parallelize() missing 1 required positional argument: 'c'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [13], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkContext \u001b[38;5;28;01mas\u001b[39;00m sc\n\u001b[1;32m      3\u001b[0m li\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m4\u001b[39m]\n\u001b[0;32m----> 4\u001b[0m rdd1 \u001b[38;5;241m=\u001b[39m sc\u001b[38;5;241m.\u001b[39mparallelize(li)\n\u001b[1;32m      5\u001b[0m row_rdd \u001b[38;5;241m=\u001b[39m rdd1\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m x: Row(x))\n\u001b[1;32m      6\u001b[0m df\u001b[38;5;241m=\u001b[39msqlContext\u001b[38;5;241m.\u001b[39mcreateDataFrame(row_rdd,[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumbers\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mTypeError\u001b[0m: parallelize() missing 1 required positional argument: 'c'"
     ]
    }
   ],
   "source": [
    "    from pyspark.sql import Row\n",
    "    from pyspark import SparkContext as sc\n",
    "    li=[1,2,3,4]\n",
    "    rdd1 = sc.parallelize(li)\n",
    "    row_rdd = rdd1.map(lambda x: Row(x))\n",
    "    df=sqlContext.createDataFrame(row_rdd,['numbers']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4579dfa9-d668-4a1f-8d18-bd84d3cf4b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| _1| _2|\n",
      "+---+---+\n",
      "|  1|one|\n",
      "|  2|two|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdd = spark_ctxt.parallelize([(1, \"one\"),(2,\"two\")])\n",
    "dffFromParalleize=rdd.toDF()\n",
    "dffFromParalleize.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3d2337-c183-48af-83e1-5af92eb78dbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55364226-6450-4842-b60f-5198de4cee0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField. StringType, IntergerType\n",
    "\n",
    "schema = StructType([ \\\n",
    "         StructField\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "268bdd45-9ce7-4c3f-b777-218aa671921d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'row' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m Song \u001b[38;5;241m=\u001b[39m row(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124martist\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwell_known_lyrics\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m songs \u001b[38;5;241m=\u001b[39m [Song(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMary Had a Little Lamb\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMary Tyler\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMary had a little lamb Little lamb, little lamb Mary had a little lamb It\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms fleece was white as snow\u001b[39m\u001b[38;5;124m\"\u001b[39m), \n\u001b[1;32m      3\u001b[0m          Song(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSomebody\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms Watching Me\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRockwell\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI always feel like somebody\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms watching me And I have no privacy (oh, oh)\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m      4\u001b[0m          Song(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStayin Alive\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBee Gees\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWell, you can tell by the way I use my walk I\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm a woman\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms man, no time to talk\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'row' is not defined"
     ]
    }
   ],
   "source": [
    "Song = row(\"name\", \"artist\", \"well_known_lyrics\")\n",
    "songs = [Song(\"Mary Had a Little Lamb\",\"Mary Tyler\",\"Mary had a little lamb Little lamb, little lamb Mary had a little lamb It's fleece was white as snow\"), \n",
    "         Song(\"Somebody's Watching Me\",\"Rockwell\",\"I always feel like somebody's watching me And I have no privacy (oh, oh)\"),\n",
    "         Song(\"Stayin Alive\", \"Bee Gees\", \"Well, you can tell by the way I use my walk I'm a woman's man, no time to talk\")]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e52d28-266d-4fe5-a5cf-e9cafab02ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "songsDF.withColumn('stadiumWordCount', F.size(F.split(F.col('stadium'), ' '))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec965c25-5499-4615-a6f8-c69bd46f3915",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
